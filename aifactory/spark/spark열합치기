import pandas as pd

# read in the two CSV files
dir1 = './_data/spark/TEST_AWS/'
dir2 = './_data/spark/TEST_INPUT/'
csv_save = './_data/spark/'

# df1 = pd.read_csv(dir2 + 'test_input.csv')
# df2 = pd.read_csv(dir1 + 'test_aws.csv')

# # print(df1.columns)
# # print(df2.columns)
# # drop duplicates from the common column in both dataframes
# df1 = df1.drop_duplicates(subset=['common_column'])
# df2 = df2.drop_duplicates(subset=['common_column'])
# # join the two dataframes using a common column
# merged_df = pd.merge(df1, df2, on='common_column')

# # write the merged dataframe to a new CSV file
# merged_df.to_csv(csv_save + 'merged_file.csv', index=False)

df1 = pd.read_csv(dir1 + 'test_aws.csv')
df2 = pd.read_csv(dir2 + 'test_input.csv')

# drop duplicates from the common column in both dataframes
print(df1.columns)
print(df2.columns)

# merge the two dataframes on the common column
merged_df = pd.merge(df1, df2, on='common_column')

# write the merged dataframe to a new CSV file
merged_df.to_csv(csv_save + 'merged_file.csv', index=False)